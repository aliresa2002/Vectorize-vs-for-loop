{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN Forward and Back Propagation: Computation speed \"for-loop\" vs. vectorized:\n",
    "Bellow, there are two implementations of Andrew Ng's Coursera Intro Machine Learning homework#5 about using Artificial Neural Networks for handwritten digit recognition using a set of data from MNIST. The first code follows the exact homework instructions (only in Python instead of MATLAB) which performs forward and back propagation on the network to calculate the cost function and the gradient vector iterating through the entire training set with a for-loop. The second code is the vectorized implementation of the same, without using a for-loop. When I wrote both codes I also attempted to answer the question: which code will run faster? Here, it looks like the two functions perform more or less similarly, however, that seems to be the case only for Jupyter (I wonder why?). Running both codes on other compliers or IDEs like Spyder on my computer, the vectorized implementation was a lot faster (4 to 6 times faster), as Andrew also mentioned in one of the earlier lessons. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import the libraries, modules and functions we'll need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import genfromtxt as gft\n",
    "import scipy.optimize as opt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with importing the data and preparing them for the calculations:\n",
    "A quick note about our y vector: The data was taken directly from that of the homework which was modified to match MATLAB indexing (which starts with one instead of zero) and therefore, 10 was used to represent 0 so that other classes are represented by the same number as their labels, we are now turning 10s back to 0s for the same purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "read = lambda name : gft(name, delimiter = ',') #A function for reading our CSV source files\n",
    "X = read('Xmatrix.txt')\n",
    "y = read('ymatrix.txt')\n",
    "\n",
    "y[:500]= 0 #First 500 samples were all 10s representing zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now implement our sigmoid, sigmoid gradient and forward propagation functions which will be later used in our cost function. There is also a second forward propagation function defined bellow (fpropX) which takes a matrix rather than a vector and can be used for the vectorized cost function as well as testing the optimization results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "m  = np.shape(X)[0] #number of training examples\n",
    "\n",
    "sigmoid = lambda z: 1.0 / (1+np.exp(-z))\n",
    "\n",
    "sigmoidGradient = lambda z : sigmoid(z) * (1-sigmoid(z))\n",
    "\n",
    "\n",
    "\n",
    "def fprop(Theta1,Theta2,X): #Forward Propagation Function, X is a vector\n",
    "    \n",
    "    z2 = Theta1 @ X\n",
    "    \n",
    "    a2 = sigmoid(z2)\n",
    "    \n",
    "    a2 = np.vstack(((np.ones((1,np.shape(a2)[1]))), a2))\n",
    "    \n",
    "    z3 = Theta2 @ a2\n",
    "    \n",
    "    a3 = sigmoid(z3)\n",
    "    \n",
    "    return a3, a2, z2\n",
    "\n",
    "\n",
    "def fpropX(Theta1,Theta2,X): #Forward Propagation Function, X is a matrix\n",
    "    \n",
    "    z2 = Theta1 @ X.T\n",
    "    \n",
    "    a2 = sigmoid(z2)\n",
    "    \n",
    "    a2 = np.vstack(((np.ones((1,np.shape(a2)[1]))), a2))\n",
    "    \n",
    "    z3 = Theta2 @ a2\n",
    "    \n",
    "    a3 = sigmoid(z3)\n",
    "    \n",
    "    return a3, a2, z2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Calculating the cost by iterating through the training set using a for-loop:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def nnCostFxn(nn_params,\n",
    "              ils,#input layer size\n",
    "              hls,#hidden layer size\n",
    "              num_labels,# number of labels (or output classes) \n",
    "              X,y,Lambda):#Lambda is our regularization parameter\n",
    "    \n",
    "    #These cost functions take the parameters unrolled into a vector\n",
    "    Theta1 = nn_params[:(hls*(ils+1))].reshape((hls,ils+1))\n",
    "    Theta2 = nn_params[(hls*(ils+1)):].reshape((num_labels,hls+1))\n",
    "    \n",
    "    Theta1_grad = np.zeros(Theta1.shape)\n",
    "    Theta2_grad = np.zeros(Theta2.shape)\n",
    "    \n",
    "    Del1 = np.zeros(Theta1.shape)\n",
    "    Del2 = np.zeros(Theta2.shape)\n",
    "       \n",
    "    m = np.shape(X)[0]\n",
    "    \n",
    "    J = 0 \n",
    "    \n",
    "    ytemp = np.zeros((m,num_labels))\n",
    "    \n",
    "    for i in range(m):\n",
    "        ytemp[i,int(y[i])] = 1\n",
    "        \n",
    "    y = ytemp\n",
    " \n",
    "    X = np.hstack((np.ones((m,1)),X))\n",
    "    \n",
    "    for i in range(m):\n",
    "        \n",
    "        a1 = X[i].reshape((-1,1))\n",
    "        a3 , a2 , z2 = fprop(Theta1,Theta2,a1)  \n",
    "        h = a3\n",
    "        J = J +(1.0/m)*((-y[i,:] @ np.log(h)) - ((1-y[i,:]) @ np.log(1-h)))\n",
    "        \n",
    "        del3 = a3 - y[i].reshape((-1,1))\n",
    "        \n",
    "        del2 = (Theta2[:,1:].T @ del3) * sigmoidGradient(z2)\n",
    "        \n",
    "        \n",
    "        Del2 = Del2 + del3 @ a2.T\n",
    "        Del1 = Del1 + del2 @ a1.T\n",
    "        \n",
    "\n",
    "    J = J + (Lambda/(2*m)) * (np.sum(np.sum(Theta1[:,1:]**2)) + np.sum(np.sum(Theta2[:,1:]**2)))\n",
    "\n",
    "        \n",
    "    Theta1[:,0] = 0\n",
    "    Theta2[:,0] = 0\n",
    "    \n",
    "    Theta1_grad = Del1/m + (Lambda/m)*Theta1\n",
    "    Theta2_grad = Del2/m + (Lambda/m)*Theta2\n",
    "    \n",
    "    grad = np.vstack((Theta1_grad.reshape((-1,1)),Theta2_grad.reshape((-1,1))))\n",
    "    \n",
    "    return J , grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorized implementation of the Cost function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def nnCostFxnVec(nn_params,\n",
    "              ils,#input layer size\n",
    "              hls,#hidden layer size\n",
    "              num_labels,\n",
    "              X,y,Lambda):\n",
    "    Theta1 = nn_params[:(hls*(ils+1))].reshape((hls,ils+1))\n",
    "    Theta2 = nn_params[(hls*(ils+1)):].reshape((num_labels,hls+1))\n",
    "    \n",
    "    m = np.shape(X)[0]\n",
    "    \n",
    "    J = 0 \n",
    "    \n",
    "    ytemp = np.zeros((m,num_labels))\n",
    "    \n",
    "    for i in range(m):\n",
    "        ytemp[i,int(y[i])] = 1\n",
    "        \n",
    "    y = ytemp\n",
    "    \n",
    "    X = np.hstack((np.ones((m,1)),X))\n",
    "\n",
    "    a3, a2, z2 = fpropX(Theta1,Theta2,X)    \n",
    "    h = a3\n",
    "    \n",
    "    for i in range(m):\n",
    "        Jsum = (-y[i,:] @ np.log(h[:,i])) - ((1-y[i,:]) @ np.log(1-h[:,i]))\n",
    "        J = J +(1.0/m)*Jsum\n",
    "    \n",
    "    J = J + (Lambda/(2*m)) * (np.sum(np.sum(Theta1[:,1:]**2)) + np.sum(np.sum(Theta2[:,1:]**2)))\n",
    "    \n",
    "    del3= a3 - y.T\n",
    "    \n",
    "    del2 = (Theta2[:,1:].T @ del3) * sigmoidGradient(z2)\n",
    "    \n",
    "    delta2 = del3 @ a2.T\n",
    "    delta1 = del2 @ X\n",
    "    \n",
    "    Theta1[:,0] = 0\n",
    "    Theta2[:,0] = 0\n",
    "    \n",
    "    Theta1_grad = delta1/m + (Lambda/m)*Theta1\n",
    "    Theta2_grad = delta2/m + (Lambda/m)*Theta2\n",
    "    \n",
    "    grad = np.vstack((Theta1_grad.reshape((-1,1)),Theta2_grad.reshape((-1,1))))\n",
    "\n",
    "\n",
    "    \n",
    "    return J , grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating parameters with fmin_tnc, using both implementations of the cost function and timing each process separately: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters to be used for calling the functions:\n",
    "ils = 400 #input layer size\n",
    "hls = 25 #hidden layer size\n",
    "num_labels = 10 # number of lables\n",
    "Lambda = 1\n",
    "\n",
    "#random initialization of Theta0 values\n",
    "np.random.seed(1)\n",
    "initial_Theta1 = np.random.uniform(-0.12,0.12,(hls,ils+1))\n",
    "initial_Theta2 = np.random.uniform(-0.12,0.12,(num_labels,hls+1))\n",
    "nn_params = np.vstack((initial_Theta1.reshape((-1,1)),initial_Theta2.reshape((-1,1)))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timing the optimization using the for-loop cost function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "result = opt.fmin_tnc(func=nnCostFxn, x0=nn_params, args=(ils,hls,num_labels,X,y,Lambda))\n",
    "result_params = result[0]\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timing the optimization using the vectorized cost function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "startV = time.time()\n",
    "\n",
    "resultV = opt.fmin_tnc(func=nnCostFxn, x0=nn_params, args=(ils,hls,num_labels,X,y,Lambda))\n",
    "resultV_params = resultV[0]\n",
    "\n",
    "endV = time.time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Printing out the results:\n",
    "Other than the time, we also calculated and printed the accuracy of predictions for each set of calculated parameters which turn out, as expected, to be exactly the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for fmin_tnc to calculate(for-loop cost fxn): 296.913743019104 \n",
      " Accuracy: 94.84\n",
      "Time for fmin_tnc to calculate(vectorized cost fxn): 283.21832609176636 \n",
      " Accuracy: 94.84\n"
     ]
    }
   ],
   "source": [
    "Theta1 = result_params[:hls*(ils+1)].reshape((hls,ils+1))\n",
    "Theta2 = result_params[hls*(ils+1):].reshape((num_labels,hls+1))\n",
    "X = np.hstack((np.ones((m,1)),X))\n",
    "h = fpropX(Theta1,Theta2,X)[0]\n",
    "pred = h.argmax(axis=0)\n",
    "\n",
    "acc = pred == y\n",
    "    \n",
    "percacc = np.mean(acc) *100\n",
    "\n",
    "print(\"Time for fmin_tnc to calculate(for-loop cost fxn):\", end- start, \"\\n Accuracy:\", percacc)\n",
    "\n",
    "\n",
    "Theta1V = resultV_params[:hls*(ils+1)].reshape((hls,ils+1))\n",
    "Theta2V = resultV_params[hls*(ils+1):].reshape((num_labels,hls+1))\n",
    "\n",
    "hV = fpropX(Theta1V,Theta2V,X)[0]\n",
    "predV = hV.argmax(axis=0)\n",
    "\n",
    "accV = predV == y\n",
    "    \n",
    "percaccV = np.mean(accV) *100\n",
    "\n",
    "print(\"Time for fmin_tnc to calculate(vectorized cost fxn):\", endV- startV, \"\\n Accuracy:\", percaccV)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
